{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Демонстрация работы татарской LLM\n",
        "\n",
        "В этом ноутбуке мы загружаем обученную модель и тестируем её способность генерировать текст на татарском языке.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "from model import GPT, GPTConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Загрузка модели и токенизатора\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Используемое устройство: mps\n"
          ]
        }
      ],
      "source": [
        "# Пути к модели и токенизатору\n",
        "model_path = \"models/best_model.pt\"\n",
        "tokenizer_path = \"tokenizer\"\n",
        "\n",
        "# Определяем устройство\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "\n",
        "print(f\"Используемое устройство: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер словаря: 8192\n",
            "BOS token: <bos> (id: 2)\n",
            "EOS token: <eos> (id: 3)\n"
          ]
        }
      ],
      "source": [
        "# Загружаем токенизатор\n",
        "tokenizer = Tokenizer.from_file(str(Path(tokenizer_path) / \"tokenizer.json\"))\n",
        "\n",
        "# Загружаем конфигурацию токенизатора\n",
        "with open(Path(tokenizer_path) / \"config.json\", 'r') as f:\n",
        "    tok_config = json.load(f)\n",
        "\n",
        "print(f\"Размер словаря: {tokenizer.get_vocab_size()}\")\n",
        "print(f\"BOS token: {tok_config['bos_token']} (id: {tok_config['bos_token_id']})\")\n",
        "print(f\"EOS token: {tok_config['eos_token']} (id: {tok_config['eos_token_id']})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Конфигурация модели:\n",
            "  Слои: 6\n",
            "  Heads: 8\n",
            "  Размерность: 512\n",
            "  Размер контекста: 256\n",
            "  Rotary Embeddings: True\n",
            "\n",
            "Лучший val_loss: 8.5254\n"
          ]
        }
      ],
      "source": [
        "# Загружаем чекпоинт модели\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "# Создаём конфигурацию модели\n",
        "model_config = GPTConfig(**checkpoint['config'])\n",
        "\n",
        "print(\"Конфигурация модели:\")\n",
        "print(f\"  Слои: {model_config.n_layer}\")\n",
        "print(f\"  Heads: {model_config.n_head}\")\n",
        "print(f\"  Размерность: {model_config.n_embd}\")\n",
        "print(f\"  Размер контекста: {model_config.block_size}\")\n",
        "print(f\"  Rotary Embeddings: {model_config.use_rotary}\")\n",
        "print(f\"\\nЛучший val_loss: {checkpoint['best_val_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 23,109,632 (23.11M)\n",
            "Модель успешно загружена!\n"
          ]
        }
      ],
      "source": [
        "# Создаём и загружаем модель\n",
        "model = GPT(model_config)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Модель успешно загружена!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Функция генерации текста\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Функция генерации готова!\n"
          ]
        }
      ],
      "source": [
        "def generate_text(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 40,\n",
        "    num_samples: int = 1\n",
        "):\n",
        "    \"\"\"\n",
        "    Генерирует текст на основе промпта\n",
        "    \"\"\"\n",
        "    # Токенизируем промпт\n",
        "    encoded = tokenizer.encode(prompt)\n",
        "    idx = torch.tensor([encoded.ids], dtype=torch.long, device=device)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for _ in range(num_samples):\n",
        "        # Генерируем\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                idx,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k\n",
        "            )\n",
        "        \n",
        "        # Декодируем\n",
        "        generated_ids = generated[0].tolist()\n",
        "        # Убираем специальные токены\n",
        "        generated_ids = [t for t in generated_ids if t not in [tok_config['bos_token_id'], tok_config['eos_token_id']]]\n",
        "        text = tokenizer.decode(generated_ids)\n",
        "        results.append(text)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Функция генерации готова!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тест 1: Продолжение предложения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Промпт: Казан шәһәре\n",
            "\n",
            "Вариант 1: Казан шәһәре\n",
            "--------------------------------------------------------------------------------\n",
            "Вариант 2: Казан шәһәре ,\n",
            "--------------------------------------------------------------------------------\n",
            "Вариант 3: Казан шәһәре . “\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Казан шәһәре\"\n",
        "results = generate_text(prompt, max_new_tokens=30, temperature=0.8, num_samples=3)\n",
        "\n",
        "print(f\"Промпт: {prompt}\\n\")\n",
        "for i, text in enumerate(results, 1):\n",
        "    print(f\"Вариант {i}: {text}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тест 2: Разные промпты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Промпт: Татарстанның башкаласы\n",
            "================================================================================\n",
            "Вариант 1: Татарстанның башкаласы . , ,\n",
            "--------------------------------------------------------------------------------\n",
            "Вариант 2: Татарстанның башкаласы дип\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Промпт: Татар теле\n",
            "================================================================================\n",
            "Вариант 1: Татар теле\n",
            "--------------------------------------------------------------------------------\n",
            "Вариант 2: Татар теле һәм\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Промпт: Казан университеты\n",
            "================================================================================\n",
            "Вариант 1: Казан университеты һәм\n",
            "--------------------------------------------------------------------------------\n",
            "Вариант 2: Казан университеты\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Примеры вопросов на татарском\n",
        "prompts = [\n",
        "    \"Татарстанның башкаласы\",\n",
        "    \"Татар теле\",\n",
        "    \"Казан университеты\",\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\nПромпт: {prompt}\")\n",
        "    print(\"=\" * 80)\n",
        "    results = generate_text(prompt, max_new_tokens=40, temperature=0.7, num_samples=2)\n",
        "    for i, text in enumerate(results, 1):\n",
        "        print(f\"Вариант {i}: {text}\")\n",
        "        print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Тест 3: Разные формулировки одного вопроса\n",
        "\n",
        "Проверим, отвечает ли модель последовательно на похожие вопросы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Промпт: Казан\n",
            "================================================================================\n",
            "Казан\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Промпт: Казан шәһәре\n",
            "================================================================================\n",
            "Казан шәһәре\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Промпт: Татарстанның башкаласы Казан\n",
            "================================================================================\n",
            "Татарстанның башкаласы Казан .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Разные формулировки про Казань\n",
        "prompts = [\n",
        "    \"Казан\",\n",
        "    \"Казан шәһәре\",\n",
        "    \"Татарстанның башкаласы Казан\",\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\nПромпт: {prompt}\")\n",
        "    print(\"=\" * 80)\n",
        "    result = generate_text(prompt, max_new_tokens=50, temperature=0.6, num_samples=1)[0]\n",
        "    print(result)\n",
        "    print(\"-\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
