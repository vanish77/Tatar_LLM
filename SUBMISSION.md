# Чеклист для сдачи работы

Этот документ поможет убедиться, что всё готово к сдаче домашнего задания.

## ? Что нужно сдать

### 1. Ссылка на репозиторий или ноутбук с полным кодом

**Что включить в репозиторий**:

```
? 01_prepare_data.py        - Скрипт загрузки и подготовки данных
? 02_train_tokenizer.py     - Скрипт обучения токенизатора
? 03_train.py               - Скрипт обучения модели
? model.py                  - Архитектура GPT модели
? config.py                 - Конфигурационный файл
? utils.py                  - Утилиты для работы с моделью
? requirements.txt          - Зависимости проекта
? README.md                 - Основная документация
? ARCHITECTURE.md           - Описание архитектуры
? EXAMPLES.md               - Примеры работы модели
? .gitignore                - Игнорируемые файлы
```

**НЕ включать** (слишком большие файлы):
```
? data/ (корпус)
? models/ (обученные модели - загрузить отдельно)
? tokenizer/ (загрузить отдельно)
```

### 2. Обученная модель (чекпоинт)

**Варианты сдачи**:

**Вариант A: Hugging Face Hub** (рекомендуется)
```bash
# Установить huggingface_hub
pip install huggingface_hub

# Загрузить модель
python -c "
from huggingface_hub import HfApi
api = HfApi()
api.upload_folder(
    folder_path='models',
    repo_id='your-username/tatar-gpt',
    repo_type='model'
)
"
```

**Вариант B: Google Drive / Яндекс.Диск**
- Загрузить `models/best_model.pt`
- Предоставить публичную ссылку

**Вариант C: GitHub Releases**
- Создать Release в GitHub
- Приложить `best_model.pt` как asset

### 3. Демонстрационный ноутбук с инференсом

```
? demo_inference.ipynb      - Jupyter ноутбук с примерами
```

**Что должно быть в ноутбуке**:
- ? Загрузка модели и токенизатора
- ? Функция генерации текста
- ? Минимум 3-5 примеров генерации
- ? Разные формулировки одного вопроса
- ? Анализ качества (перплексия, температура)

## ?? Проверка требований

### Обязательные требования

- [ ] **LLM обучена полностью с нуля** 
  - ? Да, архитектура в `model.py` реализована с нуля
  - ? Веса инициализированы случайно, не используются предобученные модели

- [ ] **Непопулярный язык**
  - ? Татарский язык (~5 млн носителей, не в топ-50)
  - ? Корпус взят с Leipzig Wortschatz

- [ ] **Архитектура decoder-only**
  - ? GPT архитектура (только decoder)
  - ? Autoregressive генерация
  - ? Causal attention masking

- [ ] **Длина контекста ? 256 символов**
  - ? `block_size = 256` токенов
  - ? Это примерно 200-300 символов татарского текста

- [ ] **Модель генерирует адекватный текст**
  - ? Проверяется в `demo_inference.ipynb`
  - ? Примеры в `EXAMPLES.md`

### Бонусные пункты (реализовано!)

- [x] **Rotary Position Embeddings** 
  - ? Реализовано в `model.py` (класс `RotaryPositionalEmbedding`)
  - ? `use_rotary=True` в конфигурации

- [x] **Mixed Precision Training**
  - ? Реализовано в `03_train.py` (для CUDA)
  - ? Использует `torch.amp.autocast`

- [x] **Gradient Checkpointing** (опционально)
  - ? Реализовано в `03_train.py`
  - ? `use_gradient_checkpointing` flag

- [x] **MPS support** (для Apple Silicon)
  - ? Автоматическое определение устройства
  - ? Работает на M1/M2 Mac

- [x] **Эксперименты с архитектурой**
  - ? Различные конфигурации в `config.py`
  - ? `tiny()`, `medium()`, `large()` варианты

- [ ] **Flash Attention** (опционально)
  - ??  Поддержка добавлена, но по умолчанию выключена
  - ??  Можно включить: `use_flash_attention=True`

## ?? Тестирование перед сдачей

### 1. Проверьте, что модель обучена

```bash
# Должны существовать файлы:
ls models/best_model.pt
ls models/final_model.pt
ls tokenizer/tokenizer.json
```

### 2. Запустите быстрый тест

```bash
python quick_test.py
```

Ожидаемый результат: модель генерирует татарский текст без ошибок.

### 3. Проверьте ноутбук

```bash
jupyter notebook demo_inference.ipynb
```

Запустите все ячейки - они должны выполниться без ошибок.

### 4. Проверьте разные формулировки

В ноутбуке убедитесь, что модель:
- ? Отвечает на "Казан"
- ? Отвечает на "Казан ш???ре"
- ? Отвечает на "Татарстанны? башкаласы"
- ? Ответы связаны между собой (про Казань)

### 5. Проверьте перплексию

```python
from utils import calculate_perplexity, load_model, load_tokenizer

model, _ = load_model("models/best_model.pt")
tokenizer, _ = load_tokenizer()

ppl = calculate_perplexity(
    model, tokenizer,
    "Казан ш???ре Татарстан Республикасыны? башкаласы."
)
print(f"Perplexity: {ppl}")
```

Ожидаемое значение: 15-30 (чем ниже, тем лучше).

## ?? Описание для преподавателя

**Шаблон README для репозитория**:

```markdown
# Татарская LLM - Домашнее задание по NLP

## Автор
[Ваше имя]

## Описание
Обучение GPT модели (decoder-only) с нуля на татарском языке.

## Характеристики модели
- **Язык**: Татарский (~5 млн носителей)
- **Архитектура**: GPT (6 слоёв, 8 heads, 512 dim)
- **Параметры**: ~23 миллиона
- **Контекст**: 256 токенов
- **Токенизатор**: BPE (8192 tokens)
- **Корпус**: Leipzig Wortschatz (~100 МБ)

## Реализованные бонусы
? Rotary Position Embeddings
? Mixed Precision Training
? Gradient Checkpointing
? MPS support (Apple Silicon)

## Быстрый старт
См. [QUICKSTART.md](QUICKSTART.md)

## Примеры работы
См. [EXAMPLES.md](EXAMPLES.md)

## Демонстрация
Запустите [demo_inference.ipynb](demo_inference.ipynb)

## Загрузка модели
[Ссылка на Hugging Face / Google Drive / GitHub Release]
```

## ?? Как сдавать

### Шаг 1: Создайте GitHub репозиторий

```bash
cd /Users/ivanevgenyevich/Documents/Study/ММОТ_NLP
git init
git add .
git commit -m "Initial commit: Tatar LLM project"
git remote add origin https://github.com/your-username/tatar-llm.git
git push -u origin main
```

### Шаг 2: Загрузите модель

Выберите один из способов выше (Hugging Face / Google Drive / GitHub Release)

### Шаг 3: Обновите README

Добавьте в README.md ссылку на загруженную модель.

### Шаг 4: Сдайте преподавателю

Предоставьте:
1. ? Ссылку на GitHub репозиторий
2. ? Ссылку на обученную модель
3. ? Ссылку на demo_inference.ipynb (или GitHub с ним)

## ?? Важные замечания

### Что точно проверят:

1. **Модель обучена с нуля**
   - Не используются предобученные веса
   - Код обучения присутствует

2. **Генерация адекватного текста**
   - Запустят ваш ноутбук
   - Проверят разные промпты
   - Возможно, проверят Google Translate

3. **Разные формулировки**
   - Должны быть примеры похожих вопросов
   - Ответы должны быть связанными

### Типичные ошибки (избегайте!):

? Модель просто запоминает обучающие данные (переобучение)
? Решение: показать генерацию на новых промптах

? Модель генерирует один и тот же ответ на всё
? Решение: проверить temperature, обучить дольше

? Ноутбук не запускается из-за отсутствия файлов
? Решение: предоставить модель и инструкции по запуску

? README не объясняет, как запустить
? Решение: QUICKSTART.md с пошаговой инструкцией

## ?? Финальный чеклист

Перед сдачей убедитесь:

- [ ] ? Код загружен в репозиторий
- [ ] ? README.md заполнен
- [ ] ? Модель обучена и сохранена
- [ ] ? Модель загружена (HF/GDrive/GH)
- [ ] ? demo_inference.ipynb работает
- [ ] ? Есть примеры с разными формулировками
- [ ] ? Примеры показывают понимание контекста
- [ ] ? Указаны реализованные бонусы
- [ ] ? requirements.txt актуален
- [ ] ? .gitignore настроен (не загружаются большие файлы)

## ?? Дополнительные советы

1. **Сделайте видео** (опционально, но круто!)
   - Запись экрана с запуском ноутбука
   - Показываете генерацию на разных промптах
   - 2-3 минуты

2. **Напишите блог-пост** (для портфолио)
   - Medium / Habr / личный блог
   - Опишите процесс и результаты
   - Покажите работодателям!

3. **Создайте демо** (впечатляет!)
   - Gradio / Streamlit интерфейс
   - Развернуть на HuggingFace Spaces
   - Интерактивное взаимодействие с моделью

Удачи! ??


